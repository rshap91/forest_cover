{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = lambda x : \"{:,.2f}\".format(x)\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 54), (565892, 55), (565892, 115))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw train and test\n",
    "train = pd.read_csv('clean_data/train.csv', dtype={'Id':str})\n",
    "test = pd.read_csv('clean_data/test.csv', dtype={'Id':str})\n",
    "\n",
    "all_test_df = pd.read_csv('clean_data/all_test_df.csv')\n",
    "\n",
    "train.shape, test.shape, all_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 200), (565892, 200))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w engineered features\n",
    "poly_train = pd.read_csv('clean_data/train_poly_final.csv')\n",
    "poly_test = pd.read_csv('clean_data/test_poly_final.csv')\n",
    "poly_train.shape, poly_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "with open('models/lreg_results.json') as f:\n",
    "    lreg_results = json.load(f)\n",
    "with open('models/lda_results.json') as f:    \n",
    "    lda_results = json.load(f)\n",
    "with open('models/knn_results.json') as f:    \n",
    "    knn_results = json.load(f)\n",
    "with open('models/svm_results.json') as f:    \n",
    "    svm_results = json.load(f)\n",
    "\n",
    "with open('models/rf_results.json') as f:\n",
    "    rf_results = json.load(f)\n",
    "with open('models/et_results.json') as f:    \n",
    "    et_results = json.load(f)\n",
    "with open('models/mlp_results.json') as f:    \n",
    "    mlp_results = json.load(f)\n",
    "with open('models/lgbm_results.json') as f:    \n",
    "    lgbm_results = json.load(f)\n",
    "with open('models/xgb_results.json') as f:    \n",
    "    xgb_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce in-memory size of pandas dataframe by compressing dtypes\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        #else: df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 6.23 MB\n",
      "Memory usage after optimization is: 1.01 MB\n",
      "Decreased by 83.8%\n",
      "Memory usage of dataframe is 237.46 MB\n",
      "Memory usage after optimization is: 38.32 MB\n",
      "Decreased by 83.9%\n",
      "Memory usage of dataframe is 496.50 MB\n",
      "Memory usage after optimization is: 104.70 MB\n",
      "Decreased by 78.9%\n",
      "Memory usage of dataframe is 23.07 MB\n",
      "Memory usage after optimization is: 5.77 MB\n",
      "Decreased by 75.0%\n",
      "Memory usage of dataframe is 863.48 MB\n",
      "Memory usage after optimization is: 212.09 MB\n",
      "Decreased by 75.4%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "all_test_df = reduce_mem_usage(all_test_df)\n",
    "\n",
    "poly_train = reduce_mem_usage(poly_train)\n",
    "poly_test = reduce_mem_usage(poly_test)\n",
    "\n",
    "\n",
    "ytrain = train['Cover_Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure the proper way to do this.\n",
    "\n",
    " - training all base_estimators on whole dataset.\n",
    " - generating probabilities on the same dataset\n",
    " - training meta estimators on those probs -whole dataset\n",
    " - submitting test predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_results\n",
    "\n",
    "all_results = {\n",
    "    LogisticRegression: lreg_results,\n",
    "    LinearDiscriminantAnalysis: lda_results,\n",
    "    KNeighborsClassifier: knn_results,\n",
    "    SVC: svm_results,\n",
    "    RandomForestClassifier: rf_results,\n",
    "    ExtraTreesClassifier: et_results,\n",
    "    MLPClassifier: mlp_results,\n",
    "    LGBMClassifier: lgbm_results,\n",
    "    XGBClassifier: xgb_results\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mods = [\n",
    "    l(**{k:v for k,v in p.items() if k!='accuracy'}) for l, p in all_results.items()\n",
    "]\n",
    "mods.pop(3) # SVC\n",
    "# so as to get predict_proba\n",
    "mods.append(SVC(probability=True, **{k:v for k,v in all_results[SVC].items() if k!='accuracy'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross_validate meta-estimator\n",
    "\n",
    "# def cv_meta_est(est, params=None):\n",
    "#     \"Poorly written function\"\n",
    "#     nclasses = ytrain.nunique()\n",
    "#     params = params or {}\n",
    "\n",
    "#     scores = []\n",
    "    \n",
    "#     # split into train and test folds\n",
    "#     # large test size because we are going to split that again for meta-estimator\n",
    "#     cv = StratifiedShuffleSplit(NCV,test_size=0.4, random_state=seed)\n",
    "#     splits = cv.split(poly_train, ytrain)\n",
    "    \n",
    "#     # iterate through folds\n",
    "#     for train_idxs, test_idxs in splits:\n",
    "#         all_probs = [] # list of dfs w class probs for given est\n",
    "#         # generate predict_probas from each estimator\n",
    "#         for m in mods:\n",
    "#             name = str(m.__class__).split('.')[-1].strip('>').strip(\"'\")\n",
    "#             print(name)\n",
    "#             m.fit(poly_train.iloc[train_idxs], ytrain.iloc[train_idxs])\n",
    "                \n",
    "            \n",
    "#             probs = pd.DataFrame(m.predict_proba(poly_train.iloc[test_idxs]), columns = [name + '_CLASS_' + str(i+1) for i in range(nclasses)])\n",
    "#             all_probs.append(probs)\n",
    "\n",
    "#         # this is now our input features into next level models\n",
    "#         all_prob_df = pd.concat(all_probs, axis=1)\n",
    "#         # cv meta-estimator on predicted probs for test set\n",
    "#         meta_cv = StratifiedShuffleSplit(3, test_size=0.2)\n",
    "#         mn_accuracy = np.mean(cross_val_score(est(**params), all_prob_df, ytrain[test_idxs], cv=meta_cv, scoring='accuracy'))\n",
    "#         scores.append(mn_accuracy)\n",
    "#         print(mn_accuracy)\n",
    "#     return scores\n",
    "        \n",
    "# scores = cv_meta_est(LogisticRegression)\n",
    "# scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_meta_inputs(mods):\n",
    "    \"mods: List of Unfitted Paramaterized Estimators\"\n",
    "    nclasses = ytrain.nunique()\n",
    "    all_probs_train = [] # list of dfs w class probs for given est\n",
    "    all_probs_test = []\n",
    "    # generate predict_probas from each estimator\n",
    "    for m in mods:\n",
    "        name = str(m.__class__).split('.')[-1].strip('>').strip(\"'\")\n",
    "        print(name)\n",
    "        m.fit(poly_train, ytrain)\n",
    "\n",
    "\n",
    "        probs_train = pd.DataFrame(m.predict_proba(poly_train), columns = [name + '_CLASS_' + str(i+1) for i in range(nclasses)])\n",
    "        probs_test = pd.DataFrame(m.predict_proba(poly_test), columns = [name + '_CLASS_' + str(i+1) for i in range(nclasses)])\n",
    "        all_probs_train.append(probs_train)\n",
    "        all_probs_test.append(probs_test)\n",
    "\n",
    "    # this is now our input features into next level models\n",
    "    all_prob_train_df = pd.concat(all_probs_train, axis=1)\n",
    "    all_prob_test_df = pd.concat(all_probs_test, axis=1)\n",
    "\n",
    "    return all_prob_train_df, all_prob_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_probs_train_df, all_probs_test_df = gen_meta_inputs(mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_probs_train_df.to_csv('clean_data/all_probs_train_df.csv', index=False)\n",
    "all_probs_test_df.set_index(test.Id).to_csv('clean_data/all_probs_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 7.27 MB\n",
      "Memory usage after optimization is: 1.82 MB\n",
      "Decreased by 75.0%\n",
      "Memory usage of dataframe is 276.31 MB\n",
      "Memory usage after optimization is: 70.16 MB\n",
      "Decreased by 74.6%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((15120, 63), (565892, 64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs_train_df = pd.read_csv('clean_data/all_probs_train_df.csv')\n",
    "all_probs_test_df = pd.read_csv('clean_data/all_probs_test_df.csv')\n",
    "\n",
    "all_probs_train_df = reduce_mem_usage(all_probs_train_df)\n",
    "all_probs_test_df = reduce_mem_usage(all_probs_test_df)\n",
    "\n",
    "all_probs_train_df.shape, all_probs_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['KNeighborsClassifier_CLASS_1',\n",
       "  'KNeighborsClassifier_CLASS_2',\n",
       "  'KNeighborsClassifier_CLASS_3',\n",
       "  'KNeighborsClassifier_CLASS_4',\n",
       "  'KNeighborsClassifier_CLASS_5',\n",
       "  'KNeighborsClassifier_CLASS_6',\n",
       "  'KNeighborsClassifier_CLASS_7'],\n",
       " ['LinearDiscriminantAnalysis_CLASS_1',\n",
       "  'LinearDiscriminantAnalysis_CLASS_2',\n",
       "  'LinearDiscriminantAnalysis_CLASS_3',\n",
       "  'LinearDiscriminantAnalysis_CLASS_4',\n",
       "  'LinearDiscriminantAnalysis_CLASS_5',\n",
       "  'LinearDiscriminantAnalysis_CLASS_6',\n",
       "  'LinearDiscriminantAnalysis_CLASS_7'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These columns seem to be hurting the models\n",
    "kn_cols = all_probs_train_df.columns[all_probs_train_df.columns.str.startswith('KNeighborsClassifier')].tolist()\n",
    "lda_cols = all_probs_train_df.columns[all_probs_train_df.columns.str.startswith('LinearDiscriminantAnalysis')].tolist()\n",
    "kn_cols, lda_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = all_probs_test_df.Id\n",
    "test_x = all_probs_test_df.drop('Id',axis=1)\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick.shapiro/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# 79%\n",
    "lgbm = LGBMClassifier(n_estimators=100, max_depth = 10, colsample_bytree=0.1, random_state=seed)\n",
    "lgbm.fit(all_probs_train_df.drop(kn_cols,axis=1).values, ytrain)\n",
    "preds_gbm = lgbm.predict(test_x.drop(kn_cols,axis=1).values)\n",
    "pd.DataFrame({'Id': test_id, 'Cover_Type':preds_gbm}).to_csv('Submissions/Stacked_LGBM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick.shapiro/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#78%\n",
    "xgb = XGBClassifier(n_estimators=10, max_depth = 10, colsample_bytree=0.1, objective='multi:softmax', random_state=seed)\n",
    "xgb.fit(all_probs_train_df.drop(kn_cols,axis=1).values, ytrain)\n",
    "preds_xgb = xgb.predict(test_x.drop(kn_cols,axis=1).values)\n",
    "pd.DataFrame({'Id': test_id, 'Cover_Type':preds_xgb}).to_csv('Submissions/Stacked_xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 76%\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth = 10, max_features=0.1, random_state=seed)\n",
    "rf.fit(all_probs_train_df.values, ytrain)\n",
    "preds_rf = rf.predict(test_x.values)\n",
    "pd.DataFrame({'Id': test_id, 'Cover_Type':preds_rf}).to_csv('Submissions/Stacked_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%\n",
    "lreg = LogisticRegression(penalty='l1', C=0.25, random_state=seed)\n",
    "lreg.fit(all_probs_train_df.values, ytrain)\n",
    "preds_lreg = lreg.predict(test_x.values)\n",
    "pd.DataFrame({'Id': test_id, 'Cover_Type':preds_lreg}).to_csv('Submissions/Stacked_lreg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(all_probs_train_df.values, ytrain)\n",
    "preds_svc = svc.predict(test_x.values)\n",
    "pd.DataFrame({'Id': test_id, 'Cover_Type':preds_svc}).to_csv('Submissions/Stacked_svc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Id':test_id, \n",
    "    'Cover_Type': np.ravel(stats.mode(np.vstack([preds_gbm,preds_xgb, preds_rf]).T, 1)[0])\n",
    "}).to_csv('Submissions/stacked_trees_maj_vote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
