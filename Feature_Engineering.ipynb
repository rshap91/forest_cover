{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from feature_selector import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 80\n",
    "pd.options.display.float_format = lambda x : \"{:,.2f}\".format(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 54), (565892, 55))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('clean_data/train.csv', dtype={'Id':str})\n",
    "test = pd.read_csv('clean_data/test.csv', dtype={'Id':str})\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('clean_data/train_cols.json', 'r') as f:\n",
    "    train_cols = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols.keys()\n",
    "id_cols = train_cols['id']\n",
    "cat_cols = train_cols['cat_cols']\n",
    "int_cols = train_cols['int_cols']\n",
    "target_col = train_cols['target_col']\n",
    "\n",
    "ftr_cols = int_cols + cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = train[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "    - Scale Data\n",
    "    - KMeans and DBSCAN for Cluster Labels as features\n",
    "    - Clustering On PCA/SVD/NMF\n",
    "    - NaiveBayes Probabilities as Features\n",
    "    - Run Through Feature Selector\n",
    "    - Polynomial Features\n",
    "    - Feature Selector Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "xtrain_scaled = mm.fit_transform(train[ftr_cols])\n",
    "xtest_scaled = mm.transform(test[ftr_cols])\n",
    "ytrain = train[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xtrain_scaled, columns = ftr_cols).to_csv('clean_data/xtrain_scaled.csv',index=False)\n",
    "pd.DataFrame(xtest_scaled, columns=ftr_cols).to_csv('clean_data/xtest_scaled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_train_data = {}\n",
    "km_test_data = {}\n",
    "for k in range(3,15):\n",
    "    print(k)\n",
    "    km = KMeans(k)\n",
    "    km_train = km.fit_predict(xtrain_scaled)\n",
    "    km_test = km.predict(xtest_scaled)\n",
    "    \n",
    "    km_train_data['K'+str(k)] = km_train\n",
    "    km_test_data['K'+str(k)] = km_test\n",
    "    \n",
    "    \n",
    "# make DFS\n",
    "km_train_df = pd.DataFrame(km_train_data)\n",
    "km_test_df = pd.DataFrame(km_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_train_df.to_csv('clean_data/km_train_df.csv',index=False)\n",
    "km_test_df.to_csv('clean_data/km_test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DB has no transform method so we can't use it on new data\n",
    "\n",
    "TODO: write predict method for db that just takes distances from cluster centers/avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# db_train_data = {}\n",
    "# db_test_data = {}\n",
    "# for eps in [0.1, 0.5, 1, 1.25]:\n",
    "#     for ms in range(5,50,10):\n",
    "#         print('EPS:',eps, 'MS:', ms )\n",
    "#         db = DBSCAN(eps,ms, n_jobs=-1)\n",
    "#         db_train = db.fit_predict(xtrain_scaled)\n",
    "#         db_test = db.predict(xtest_scaled)\n",
    "#         db_train_data['EPS'+str(eps)+'_MS'+str(ms)] = db_train\n",
    "#         db_test_data['EPS'+str(eps)+'_MS'+str(ms)] = db_test\n",
    "        \n",
    "# # make DFS\n",
    "# db_train_df = pd.DataFrame(db_train_data)\n",
    "# db_test_df = pd.DataFrame(db_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# db_train_df.to_csv('clean_data/db_train_df.csv',index=False)\n",
    "# db_test_df.to_csv('clean_data/db_test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD  & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 10)\n",
    "\n",
    "svd_train = svd.fit_transform(xtrain_scaled)\n",
    "svd_test = svd.transform(xtest_scaled)\n",
    "\n",
    "pca = PCA(n_components = 10)\n",
    "\n",
    "pca_train = pca.fit_transform(xtrain_scaled)\n",
    "pca_test = pca.transform(xtest_scaled)\n",
    "\n",
    "\n",
    "# make dataframes\n",
    "svd_train_df = pd.DataFrame(svd_train, columns = ['svd'+str(i) for i in range(svd_train.shape[1])])\n",
    "svd_test_df = pd.DataFrame(svd_test, columns = ['svd'+str(i) for i in range(svd_train.shape[1])])\n",
    "\n",
    "pca_train_df = pd.DataFrame(pca_train, columns = ['pca'+str(i) for i in range(svd_train.shape[1])])\n",
    "pca_test_df = pd.DataFrame(pca_test, columns = ['pca'+str(i) for i in range(svd_train.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_train_df.to_csv('clean_data/svd_train_df.csv',index=False)\n",
    "svd_test_df.to_csv('clean_data/svd_test_df.csv',index=False)\n",
    "pca_train_df.to_csv('clean_data/pca_train_df.csv',index=False)\n",
    "pca_test_df.to_csv('clean_data/pca_test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster On Transformed Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_svd_train_data = {}\n",
    "km_svd_test_data = {}\n",
    "for k in range(3,15):\n",
    "    print(k)\n",
    "    km = KMeans(k)\n",
    "    km_train = km.fit_predict(svd_train)\n",
    "    km_test = km.predict(svd_test)\n",
    "    \n",
    "    km_svd_train_data['SVD_K'+str(k)] = km_train\n",
    "    km_svd_test_data['SVD_K'+str(k)] = km_test\n",
    "    \n",
    "km_pca_train_data = {}\n",
    "km_pca_test_data = {}\n",
    "for k in range(3,15):\n",
    "    print(k)\n",
    "    km = KMeans(k)\n",
    "    km_train = km.fit_predict(pca_train)\n",
    "    km_test = km.predict(pca_test)\n",
    "    \n",
    "    km_pca_train_data['PCA_K'+str(k)] = km_train\n",
    "    km_pca_test_data['PCA_K'+str(k)] = km_test\n",
    "    \n",
    "\n",
    "# Make DFS\n",
    "km_svd_train_df = pd.DataFrame(km_svd_train_data)\n",
    "km_svd_test_df = pd.DataFrame(km_svd_test_data)\n",
    "\n",
    "km_pca_train_df = pd.DataFrame(km_pca_train_data)\n",
    "km_pca_test_df = pd.DataFrame(km_pca_test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_svd_train_df.to_csv('clean_data/km_svd_train_df.csv',index=False)\n",
    "km_svd_test_df.to_csv('clean_data/km_svd_test_df.csv',index=False)\n",
    "km_pca_train_df.to_csv('clean_data/km_pca_train_df.csv',index=False)\n",
    "km_pca_test_df.to_csv('clean_data/km_pca_test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# db_svd_train_data = {}\n",
    "# db_svd_test_data = {}\n",
    "# for eps in [0.1, 0.5, 1, 1.25]:\n",
    "#     for ms in range(5,50,10):\n",
    "#         print('EPS:',eps, 'MS:', ms )\n",
    "#         db = DBSCAN(eps,ms, n_jobs=-1)\n",
    "#         db_train = db.fit_predict(svd_train)\n",
    "#         db_test = db.predict(svd_test)\n",
    "        \n",
    "#         db_svd_train_data['SVD_EPS'+str(eps)+'_MS'+str(ms)] = db_train\n",
    "#         db_svd_test_data['SVD_EPS'+str(eps)+'_MS'+str(ms)] = db_test\n",
    "        \n",
    "# db_pca_train_data = {}\n",
    "# db_pca_test_data = {}\n",
    "# for eps in [0.1, 0.5, 1, 1.25]:\n",
    "#     for ms in range(5,50,10):\n",
    "#         print('EPS:',eps, 'MS:', ms )\n",
    "#         db = DBSCAN(eps,ms, n_jobs=-1)\n",
    "#         db_train = db.fit_predict(pca_train)\n",
    "#         db_test = db.predict(pca_test)\n",
    "        \n",
    "#         db_pca_train_data['PCA_EPS'+str(eps)+'_MS'+str(ms)] = db_train\n",
    "#         db_pca_test_data['PCA_EPS'+str(eps)+'_MS'+str(ms)] = db_test\n",
    "        \n",
    "\n",
    "# # Make DFS\n",
    "# db_svd_train_df = pd.DataFrame(db_svd_train_data)\n",
    "# db_svd_test_df = pd.DataFrame(db_svd_test_data)\n",
    "\n",
    "# db_pca_train_df = pd.DataFrame(db_pca_train_data)\n",
    "# db_pca_test_df = pd.DataFrame(db_pca_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# db_svd_train_df.to_csv('clean_data/db_svd_train_df.csv',index=False)\n",
    "# db_svd_test_df.to_csv('clean_data/db_svd_test_df.csv',index=False)\n",
    "# db_pca_train_df.to_csv('clean_data/db_pca_train_df.csv',index=False)\n",
    "# db_pca_test_df.to_csv('clean_data/db_pca_test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClf():\n",
    "    \"\"\"\n",
    "    Uses Bernoulli NB for Binary Features,\n",
    "    Multinomial NB for integer ftrs,\n",
    "    Gaussian NB for all other ftrs.\n",
    "    \n",
    "    Final Probs is average of 3 predicted probabilities of above models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bparams={}, mparams={}, gparams={}):\n",
    "        self.bnb = BernoulliNB(**bparams)\n",
    "        self.mnb = MultinomialNB(**mparams)\n",
    "        self.gnb = GaussianNB(**gparams)\n",
    "        \n",
    "        \n",
    "    def get_cols(self, data):\n",
    "        bern_cols = data.columns[data.apply(lambda col: col.nunique() == 2)].tolist()\n",
    "        mult_cols = data.columns[(data.dtypes==int) & ~data.columns.isin(bern_cols)].tolist()\n",
    "        gaus_cols = data.columns[~data.columns.isin(bern_cols+mult_cols)]\n",
    "        assert len(bern_cols) + len(mult_cols) + len(gaus_cols) == data.shape[1]\n",
    "        \n",
    "        self.bern_locs = [data.columns.get_loc(b) for b in bern_cols]\n",
    "        self.mult_locs = [data.columns.get_loc(m) for m in mult_cols]\n",
    "        self.gaus_locs = [data.columns.get_loc(g) for g in gaus_cols]\n",
    "        \n",
    "    \n",
    "    def fit(self, data, target):\n",
    "        self.get_cols(data)\n",
    "        if self.bern_locs:\n",
    "            self.bnb.fit(data.values[:,self.bern_locs], target)\n",
    "        if self.mult_locs:\n",
    "            self.mnb.fit(data.values[:,self.mult_locs], target)\n",
    "        if self.gaus_locs:\n",
    "            self.gnb.fit(data.values[:,self.gaus_locs], target)\n",
    "\n",
    "    \n",
    "    def predict(self, new_data):\n",
    "        all_probs = []\n",
    "        if self.bern_locs:\n",
    "            bprobs = self.bnb.predict_proba(new_data[:,self.bern_locs])\n",
    "            all_probs.append(bprobs)\n",
    "        if self.mult_locs:\n",
    "            mprobs = self.mnb.predict_proba(new_data[:,self.mult_locs])\n",
    "            all_probs.append(mprobs)\n",
    "        if self.gaus_locs:\n",
    "            gprobs = self.gnb.predict_proba(new_data[:,self.gaus_locs])\n",
    "            all_probs.append(gprobs)\n",
    "        all_probs = np.asarray(all_probs)\n",
    "        final_probs = all_probs.mean(0)\n",
    "\n",
    "        return final_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_scaled_df = pd.DataFrame(xtrain_scaled, columns = ftr_cols)\n",
    "xtest_scaled_df = pd.DataFrame(xtest_scaled, columns = ftr_cols)\n",
    "xtrain_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbc = NaiveBayesClf()\n",
    "nbc.fit(xtrain_scaled_df, train[target_col])\n",
    "nb_train = nbc.predict(xtrain_scaled_df.values)\n",
    "nb_test = nbc.predict(xtest_scaled_df.values)\n",
    "\n",
    "nb_train_df = pd.DataFrame(nb_train, columns = ['nb_prob_'+str(i) for i in range(nb_train.shape[1])])\n",
    "nb_test_df = pd.DataFrame(nb_test, columns = ['nb_prob_'+str(i) for i in range(nb_test.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_df.to_csv('clean_data/nb_train_df.csv', index=False)\n",
    "nb_test_df.to_csv('clean_data/nb_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(ytrain.values, nb_train.argmax(1)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All New Ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xtrain_scaled_df \n",
    "# xtest_scaled_df\n",
    "\n",
    "km_train_df = pd.read_csv('clean_data/km_train_df.csv')\n",
    "svd_train_df = pd.read_csv('clean_data/svd_train_df.csv')\n",
    "pca_train_df = pd.read_csv('clean_data/pca_train_df.csv')\n",
    "km_svd_train_df = pd.read_csv('clean_data/km_svd_train_df.csv')\n",
    "km_pca_train_df = pd.read_csv('clean_data/km_pca_train_df.csv')\n",
    "nb_train_df = pd.read_csv('clean_data/nb_train_df.csv')\n",
    "\n",
    "\n",
    "km_test_df = pd.read_csv('clean_data/km_test_df.csv')\n",
    "svd_test_df = pd.read_csv('clean_data/svd_test_df.csv')\n",
    "pca_test_df = pd.read_csv('clean_data/pca_test_df.csv')\n",
    "km_svd_test_df = pd.read_csv('clean_data/km_svd_test_df.csv')\n",
    "km_pca_test_df = pd.read_csv('clean_data/km_pca_test_df.csv')\n",
    "nb_test_df = pd.read_csv('clean_data/nb_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_ftrs = [\n",
    "    xtrain_scaled_df,\n",
    "    km_train_df,\n",
    "#     db_train_df,\n",
    "    svd_train_df,\n",
    "    pca_train_df,\n",
    "    km_svd_train_df,\n",
    "    km_pca_train_df,\n",
    "#     db_svd_train_df,\n",
    "#     db_pca_train_df,\n",
    "    nb_train_df\n",
    "]\n",
    "    \n",
    "all_test_ftrs = [ \n",
    "    xtest_scaled_df,\n",
    "    km_test_df,\n",
    "#     db_test_df,\n",
    "    svd_test_df,\n",
    "    pca_test_df,\n",
    "    km_svd_test_df,\n",
    "    km_pca_test_df,\n",
    "#     db_svd_test_df,\n",
    "#     db_pca_test_df,\n",
    "    nb_test_df\n",
    "]\n",
    "\n",
    "all_train_df = pd.concat(all_train_ftrs, axis=1)\n",
    "all_test_df = pd.concat(all_test_ftrs, axis=1)\n",
    "\n",
    "all_train_df.shape, all_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_df.to_csv('clean_data/all_train_df.csv', index=False)\n",
    "all_test_df.to_csv('clean_data/all_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_df = pd.read_csv('clean_data/all_train_df.csv')\n",
    "all_test_df = pd.read_csv('clean_data/all_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N=35\n",
    "seed =1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_params = {'n_estimators': all_train_df.shape[1]-1, 'max_features':N, 'n_jobs':-1, 'random_state':seed}\n",
    "gb_params = {'n_estimators': all_train_df.shape[1]-1, 'random_state':seed, 'max_depth':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usecols = run_ftr_selection(all_train_df, ytrain, N, rf_params, gb_params)\n",
    "usecols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_subset = all_train_df[usecols.index]\n",
    "x_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(3,include_bias=False)\n",
    "train_poly = pd.DataFrame(poly.fit_transform(x_subset), columns = poly.get_feature_names())\n",
    "\n",
    "print(train_poly.shape)\n",
    "train_poly.to_csv('clean_data/train_poly_all.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 8435)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_poly = pd.read_csv('clean_data/train_poly_all.csv')\n",
    "train_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_final = train.shape[1]\n",
    "N_final = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_use_cols = run_ftr_selection(train_poly, ytrain, N_final, rf_params, skip='rfe_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_poly_final = train_poly[new_use_cols.index]\n",
    "train_poly_final.to_csv('clean_data/train_poly_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvs = []\n",
    "for c in range(3, 10):\n",
    "    acc = np.mean(cross_val_score(l, train[ftr_cols], ytrain, cv = c, scoring='accuracy'))\n",
    "    cvs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(3,10),cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l.fit(all_train_df, ytrain)\n",
    "preds = l.predict(all_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Id': test[id_cols].values, 'Cover_Type':preds}).to_csv('Submissions/base_lgbm.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
